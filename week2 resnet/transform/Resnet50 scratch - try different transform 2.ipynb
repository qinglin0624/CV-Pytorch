{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### credit:\n",
    "**Resnet code** is adopted and generalized from https://blog.csdn.net/weixin_43940163/article/details/103760294<br>\n",
    "**Setup_seed code** is adopted from https://zhuanlan.zhihu.com/p/76472385"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variations to try:\n",
    "### <font color=red>1. transform: </font> \n",
    "`CenterCrop in eval?`<br>\n",
    "If the most informative part of an image resides at the center, ignoring the marginal part can reduce noise.<br>\n",
    "`RandomResizedCrop in train? What's the optimal scale?`<br>\n",
    "It will largely enlarge the trainset with the model at a risk of underfitting.<br>\n",
    ">default: none of them\n",
    "### 2. train batchsize:\n",
    "`8`, `16`, `32`, `64`, `128`<br>\n",
    "(eval & test batchsize=64 is unchanged)<br>\n",
    ">default: 64\n",
    "### 3. learning rate:\n",
    "with batchsize = 64,<br>\n",
    "`SGD(lr=0.01)`<br>\n",
    "`SGD(lr=0.025)`<br>\n",
    "`SGD(lr=0.025)` + `StepLR(step_size=30, gamma=0.1)`<br>\n",
    "`SGD(lr=0.025)` + `StepLR(step_size=20, gamma=0.5)`<br>\n",
    ">default:<br>\n",
    "`Adam(lr=0.001)` with **no** lr_schedular\n",
    "### 4. regularization:\n",
    "Adam weight_decay = `5e-4`, `1e-4`, `5e-5`, `1e-5`\n",
    ">default: 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.RandomResizedCrop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_0 = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop((224,224), scale=(0.5, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.25, contrast=0.25, hue=0.25),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'eval': transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "transform_1 = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop((224,224), scale=(0.75, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.25, contrast=0.25, hue=0.25),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'eval': transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(TRANSFORM, TRAIN_BS):\n",
    "    data_dir = '/kaggle/input/caltech101/Caltech101/Caltech101'\n",
    "    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), TRANSFORM[x]) for x in ['train', 'eval', 'test']}\n",
    "    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64, shuffle=True, num_workers=4) for x in ['eval', 'test']}\n",
    "    dataloaders['train'] = torch.utils.data.DataLoader(image_datasets['train'], batch_size=TRAIN_BS, shuffle=True, num_workers=4)\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'eval', 'test']}\n",
    "    class_names = image_datasets['train'].classes\n",
    "    num_classes = len(class_names)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    return dataloaders, dataset_sizes, class_names, num_classes, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a batch of images\n",
    "def imshow(phase):\n",
    "    print(phase + ' images:')\n",
    "    inputs, classes = next(iter(dataloaders[phase]))\n",
    "    inp = torchvision.utils.make_grid(inputs)\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    title=[class_names[x] for x in classes]\n",
    "    plt.title(title)\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet with available versions: 18, 34, 50,101,152\n",
    "\n",
    "# block for small version of resnet\n",
    "class Block_s(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, downsample=None):\n",
    "        super(Block_s,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_planes)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)  \n",
    "        self.bn2   = nn.BatchNorm2d(out_planes)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self,x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "# block for large version of resnet\n",
    "class Block_l(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride=1, downsample=None):\n",
    "        super(Block_l,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_planes)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)  \n",
    "        self.bn2   = nn.BatchNorm2d(out_planes)\n",
    "        self.conv3 = nn.Conv2d(out_planes, out_planes*4, kernel_size=1, stride=1, bias=False)\n",
    "        self.bn3   = nn.BatchNorm2d(out_planes*4)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self,x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        \n",
    "class Resnet(nn.Module):\n",
    "    def __init__(self, version=50, num_classes=101):\n",
    "        self.v = version\n",
    "        self.v_dic = {18:[2,2,2,2,512], 34:[3,4,6,3,512], 50:[3,4,6,3,2048], 101:[3,4,23,3,2048], 152:[3,8,36,3,2048]}\n",
    "        self.in_planes = 64\n",
    "        \n",
    "        super(Resnet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxPool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1) \n",
    "        self.conv2_x = self.make_layers(self.v_dic[version][0], stride=1, planes=64)\n",
    "        self.conv3_x = self.make_layers(self.v_dic[version][1], stride=2, planes=128)\n",
    "        self.conv4_x = self.make_layers(self.v_dic[version][2], stride=2, planes=256)\n",
    "        self.conv5_x = self.make_layers(self.v_dic[version][3], stride=2, planes=512)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.fc = nn.Linear(self.v_dic[version][4], num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def make_layers(self, repeat_times, stride, planes):\n",
    "        if self.v < 50:\n",
    "            downsample = None\n",
    "            if stride!=1:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv2d(self.in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                    nn.BatchNorm2d(planes)\n",
    "                )\n",
    "            layers=[]\n",
    "            layers.append(Block_s(self.in_planes, planes, stride, downsample))\n",
    "            self.in_planes = planes\n",
    "            for i in range(repeat_times-1):\n",
    "                layers.append(Block_s(self.in_planes, planes))\n",
    "            return nn.Sequential(*layers)\n",
    "            \n",
    "        else:\n",
    "            downsample = nn.Sequential(\n",
    "                    nn.Conv2d(self.in_planes, planes*4,kernel_size=1, stride=stride, bias=False),\n",
    "                    nn.BatchNorm2d(planes*4)\n",
    "            )\n",
    "            layers=[]\n",
    "            layers.append(Block_l(self.in_planes, planes, stride, downsample))\n",
    "            self.in_planes = planes*4\n",
    "            for i in range(repeat_times-1):\n",
    "                layers.append(Block_l(self.in_planes, planes))\n",
    "            return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        # conv1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # conv2_x\n",
    "        x = self.maxPool(x)\n",
    "        x = self.conv2_x(x)\n",
    "        \n",
    "        # conv3_x\n",
    "        x = self.conv3_x(x)\n",
    "        \n",
    "        # conv4_x\n",
    "        x = self.conv4_x(x)\n",
    "        \n",
    "        # conv5_x\n",
    "        x = self.conv5_x(x)\n",
    "\n",
    "        # average pool and fc\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x =self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs, sch=False):\n",
    "    since = time.time()\n",
    "    \n",
    "    statistics = {'train':([],[]), 'eval':([],[])} # train:(loss,acc), test:(loss,acc)\n",
    "    lrs = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    iters = len(dataloaders['train'])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'eval']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "                      \n",
    "            for step,(inputs, labels) in enumerate(dataloaders[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        lrs.append((step, optimizer.param_groups[0]['lr']))\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        if sch:\n",
    "                            scheduler.step(epoch + step / iters)\n",
    "                        \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)                    \n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            statistics[phase][0].append(epoch_loss)\n",
    "            statistics[phase][1].append(float(epoch_acc))\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'eval' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best eval Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    return model, statistics, lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_accuracy(num_epochs, statistics, fig_name):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(14,4))\n",
    "    for phase,(loss,acc) in statistics.items():\n",
    "        loss_hist = [float(h) for h in loss]\n",
    "        acc_hist = [float(h) for h in acc]\n",
    "        ax1.plot(np.arange(len(loss_hist)), loss_hist, label = phase, linewidth = 0.8)\n",
    "        ax1.set(xlabel = 'Epochs', ylabel='loss',title='loss at the xth epoch');\n",
    "        ax1.legend();\n",
    "        ax2.plot(np.arange(len(acc_hist)), acc_hist, label = phase, linewidth = 0.8)\n",
    "        ax2.set(xlabel = 'Epochs', ylabel='accuracy',title='accuracy at the xth epoch');\n",
    "        ax2.legend();\n",
    "    fig.savefig(fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, details=False):  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = list(0. for i in range(num_classes))\n",
    "    class_total = list(0. for i in range(num_classes))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for count,data in enumerate(dataloaders['test']):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('test accuracy: %d %%' % (100 * correct / total))\n",
    "    if details:\n",
    "        acc_dic = {}\n",
    "        for i in range(num_classes):\n",
    "            acc = 100 * class_correct[i] / class_total[i]\n",
    "            acc_dic[class_names[i]] = acc\n",
    "            print('Accuracy of %5s : %2d %%' % (class_names[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot learning rate change\n",
    "# lr = np.array(lrs)[:,1]\n",
    "# plt.plot(np.arange(len(lr)), lr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook trys different transformations: \n",
    "**CenterCrop** in eval? **RandomResizedCrop** in train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_0\n",
    "setup_seed(20)\n",
    "dataloaders, dataset_sizes, class_names, num_classes, device = load_data(transform_0, 64)\n",
    "imshow('train')\n",
    "imshow('eval')\n",
    "\n",
    "RN_0 = Resnet(50)\n",
    "RN_0 = RN_0.to(device)\n",
    "num_epochs = 50\n",
    "criterion_0 = nn.CrossEntropyLoss()\n",
    "optimizer_0 = optim.Adam(RN_0.parameters(),lr=0.001,weight_decay=1e-5)\n",
    "cosine_lr_scheduler_0 = lr_scheduler.CosineAnnealingWarmRestarts(optimizer_0, T_0= int(1.3*num_epochs), T_mult=1, eta_min=0, last_epoch=-1)\n",
    "\n",
    "RN_0, statistics_0, _ = train_model(RN_0, criterion_0, optimizer_0, cosine_lr_scheduler_0, num_epochs=num_epochs, sch=False)\n",
    "json.dump(statistics_0, open('transform_05rcr.json', 'w'))\n",
    "visualize_accuracy(num_epochs, statistics_0, 'transform_05rcr.png')\n",
    "\n",
    "# torch.save(RN_0.state_dict(), '/kaggle/working/RN_0.pth')\n",
    "test_model(RN_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_1\n",
    "setup_seed(20)\n",
    "dataloaders, dataset_sizes, class_names, num_classes, device = load_data(transform_1, 64)\n",
    "imshow('train')\n",
    "imshow('eval')\n",
    "\n",
    "RN_1 = Resnet(50)\n",
    "RN_1 = RN_1.to(device)\n",
    "num_epochs = 50\n",
    "criterion_1 = nn.CrossEntropyLoss()\n",
    "optimizer_1 = optim.Adam(RN_1.parameters(),lr=0.001,weight_decay=1e-5)\n",
    "cosine_lr_scheduler_1 = lr_scheduler.CosineAnnealingWarmRestarts(optimizer_1, T_0= int(1.3*num_epochs), T_mult=1, eta_min=0, last_epoch=-1)\n",
    "\n",
    "RN_1, statistics_1, _ = train_model(RN_1, criterion_1, optimizer_1, cosine_lr_scheduler_1, num_epochs=num_epochs, sch=False)\n",
    "json.dump(statistics_1, open('transform_075rcr.json', 'w'))\n",
    "visualize_accuracy(num_epochs, statistics_1, 'transform_075rcr.png')\n",
    "\n",
    "# torch.save(RN_1.state_dict(), '/kaggle/working/RN_1.pth')\n",
    "test_model(RN_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
