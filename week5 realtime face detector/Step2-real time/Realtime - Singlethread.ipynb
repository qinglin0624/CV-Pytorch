{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time, Single-thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms.functional as tf\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "\n",
    "import face_recognizer\n",
    "from model import PNet,RNet,ONet,FaceNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *face_recognizer.py*, there are three functions:\n",
    "- `mtcnn`<br>\n",
    "**input**: pnet, rnet, onet, image<br>\n",
    "**output**: o_bounding_boxes, pred_ldmk, aligned_faces<br>\n",
    "Given an image and the mtcnn, it returns the predicted bounding boxes, the landmarks and the cropped faces after alignment.\n",
    "<br><br>\n",
    "- `fn`<br>\n",
    "**input**: facenet, image, embeddings, threshold<br>\n",
    "**output**: most_likely, min_dist<br>\n",
    "Given the facenet, its threshold, an image and the embeddings of candidates, it returns the name of the predicted name and the distance between the predicted embedding and the ground truth embedding.\n",
    "<br><br>\n",
    "- `decode_face`<br>\n",
    "**input**: facenet, image, threshold<br>\n",
    "**output**: decode<br>\n",
    "Given the facenet, its threshold, and an image, it returns the embedding of that image. With this function, the embedding dictionary can be created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load nets and assign the optimal threshold for facenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pnet = PNet().cuda()\n",
    "Pnet.load_state_dict(torch.load('Pnet.pth'))\n",
    "Rnet = RNet().cuda()\n",
    "Rnet.load_state_dict(torch.load('Rnet.pth'))\n",
    "Onet = ONet().cuda()\n",
    "Onet.load_state_dict(torch.load('Onet.pth'))\n",
    "Facenet = FaceNet().cuda()\n",
    "Facenet.load_state_dict(torch.load('Fnet.pth'))\n",
    "\n",
    "THRESHOLD = 3.91\n",
    "PATH = 'The Shawshank Redemption.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary of candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "frame_count = cv2.CAP_PROP_FRAME_COUNT\n",
    "frame_fps = cv2.CAP_PROP_FPS\n",
    "print(frame_count)\n",
    "print(frame_fps)\n",
    "\n",
    "andy_img = Image.open('Andy.png')\n",
    "red_img = Image.open('Red.png')\n",
    "brooks_img = Image.open('Brooks.png')\n",
    "heywood_img = Image.open('Heywood.png')\n",
    "man1_img = Image.open('Man1.png')\n",
    "man2_img = Image.open('Man2.png')\n",
    "andy_code = face_recognizer.decode_face(Facenet, andy_img, threshold=THRESHOLD)\n",
    "red_code = face_recognizer.decode_face(Facenet, red_img, threshold=THRESHOLD)\n",
    "brooks_code = face_recognizer.decode_face(Facenet, brooks_img, threshold=THRESHOLD)\n",
    "heywood_code = face_recognizer.decode_face(Facenet, heywood_img, threshold=THRESHOLD)\n",
    "man1_code = face_recognizer.decode_face(Facenet, man1_img, threshold=THRESHOLD)\n",
    "man2_code = face_recognizer.decode_face(Facenet, man2_img, threshold=THRESHOLD)\n",
    "\n",
    "embeddings = {'Andy': andy_code, 'Red': red_code, 'Brooks': brooks_code, \n",
    "              'Heywood': heywood_code, 'Man1': man1_code, 'Man2': man2_code}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n",
    "Please note that only processing is carried out here. Code for visualization is in *visualization.ipynb* and has to be run on a local computer.<br>\n",
    "All the information needed for visualization is stored in a dictionary, which contains three keys: `'face_bbox'`, `'face_ldmk'` and `'face_names'`\n",
    "The value of each key is a list whose length is equal to the number of frames of the video. For example,\n",
    "```\n",
    "statistics['face_bbox'][5] gives all the predicted bounding boxes in the fifth frame of the video\n",
    "statistics['face_names'][5] gives all the corresponding predicted names in the fifth frame of the video\n",
    "```\n",
    "However, when a local computer is equipped with GPU, it is not necessary to pre-record the video and store the dictionary. Please assign `0` to `PATH` to use the camera and umcomment the commented code below and visualize the results right away. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exit\n",
      "Complete in 1m 26s\n"
     ]
    }
   ],
   "source": [
    "face_bbox, face_ldmk, face_embd = [], [], []\n",
    "process_this_frame = True\n",
    "\n",
    "since = time.time()\n",
    "video_capture = cv2.VideoCapture(PATH)\n",
    "# w = int(video_capture.get(3))\n",
    "# h = int(video_capture.get(4))\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'DIVX')\n",
    "# outvideo = cv2.VideoWriter('output-shawshank.mp4', fourcc, 50.0, (w, h))\n",
    "\n",
    "while (video_capture.isOpened()):\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "    rgb_small_frame = small_frame[:, :, ::-1]\n",
    "\n",
    "    if process_this_frame:\n",
    "        o_bounding_boxes, pred_ldmk, aligned_faces = face_recognizer.mtcnn(Pnet, Rnet, Onet, rgb_small_frame)\n",
    "        if aligned_faces is None:\n",
    "            face_names = None\n",
    "        else:\n",
    "            pred_names = []\n",
    "            for image in aligned_faces:\n",
    "                decode, dist = face_recognizer.fn(Facenet, image, embeddings, threshold=THRESHOLD)\n",
    "                pred_names.append([decode,dist])\n",
    "\n",
    "        face_bbox.append(o_bounding_boxes)\n",
    "        face_ldmk.append(pred_ldmk)\n",
    "        face_names.append(pred_names)\n",
    "#         process_this_frame = not process_this_frame\n",
    "\n",
    "\n",
    "# #     visualize\n",
    "#     if o_bounding_boxes is not None:\n",
    "#         for (left, top, right, bottom, prob), (name, dist) in zip(o_bounding_boxes, face_names):\n",
    "\n",
    "#             top, right, bottom, left = int(4*top), int(4*right), int(4*bottom), int(4*left)\n",
    "#             cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "#             cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "#             font = cv2.FONT_HERSHEY_DUPLEX\n",
    "#             cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "    \n",
    "#     outvideo.write(frame)\n",
    "#     cv2.imshow('Video', frame)\n",
    "#     cv2.waitKey(50)\n",
    "\n",
    "print('exit')\n",
    "video_capture.release()\n",
    "spent = time.time() - since\n",
    "print('Complete in {:.0f}m {:.0f}s'.format(spent // 60, spent % 60))\n",
    "# outvideo.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478\n",
      "478\n",
      "478\n"
     ]
    }
   ],
   "source": [
    "print(len(face_bbox))\n",
    "print(len(face_ldmk))\n",
    "print(len(face_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change array to list\n",
    "for a in [face_bbox, face_ldmk]:\n",
    "    for i in range(len(a)):\n",
    "        if isinstance(a[i], np.ndarray):\n",
    "            a[i] = a[i].tolist()\n",
    "            \n",
    "statistics = {'face_bbox': face_bbox, 'face_ldmk': face_ldmk, 'face_names': face_names}\n",
    "json.dump(statistics, open('realtime-singlethread-shawshank.json', 'w', encoding='utf8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
