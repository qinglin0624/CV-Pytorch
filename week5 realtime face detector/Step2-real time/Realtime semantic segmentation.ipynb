{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-time semantic segmentation\n",
    "\n",
    "#### 10 classes vs 20 classes\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>1 Hat</td>\n",
    "        <td>1 Hat</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2 Hair</td>\n",
    "        <td>2 Hair</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>3 Arms</td>\n",
    "        <td>3 Gloves, 14 Left-arm, 15 Right-arm</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>4 Sunglasses</td>\n",
    "        <td>4 Sunglasses</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>5 UpperClothes</td>\n",
    "        <td>5 UpperClothes, 6 Dress, 7 Coat, 10 Jumpsuits, 11 Scarf</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>6 LowerClothes</td>\n",
    "        <td>8 Socks, 9 Pants, 16 Left-leg, 17 Right-leg</td>\n",
    "    </tr>    \n",
    "    <tr>\n",
    "        <td>7 Skirt</td>\n",
    "        <td>12 Skirt</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>8 Face</td>\n",
    "        <td>13 Face</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>9 Shoes</td>\n",
    "        <td>18 Left-shoe, 19 Right-shoe</td>\n",
    "    </tr>    \n",
    "</table>\n",
    "\n",
    "In this project, visualization is achieved by generating gif file instantly after processing of the video is finished. Each frame is fed into U-Net and then the output, after being equipped with a palette and then converted into an RGB image, is appended into a list. The list, which stores the segmentation of each frame, is finally used to generate the gif file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "import torchvision.transforms.functional as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from math import sqrt\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "import cv2\n",
    "import random\n",
    "import imageio\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_20 = [\n",
    "    0,   0,   0,\n",
    "    102,   0,   102,   # Hat\n",
    "    0, 0,  153,        # Hair\n",
    "    0, 128,  128,      # Glove\n",
    "    0,   255, 255,     # Sunglasses\n",
    "    255,  51, 0,       # UpperClothes\n",
    "    0, 128, 128,       # Dress\n",
    "    255, 153, 108,     # Coat\n",
    "    64,   0,   0,      # Socks\n",
    "    255,153,51,        # Pants\n",
    "    204, 51,   0,      # Jumpsuits\n",
    "    0, 153,   0,       # Scarf\n",
    "    0,   255, 0,       # Skirt\n",
    "    255, 255, 102,     # Face\n",
    "    204, 236,  255,    # Left-arm\n",
    "    255,217,179,       # Right-arm\n",
    "    0,  102, 153,      # Left-leg\n",
    "    102,204,255,       # Right-leg\n",
    "    255,0,102,         # Left-shoe\n",
    "    205,102,153        # Right-shoe\n",
    "]\n",
    "\n",
    "colors_10 = [\n",
    "    0, 0, 0,\n",
    "    9, 13, 172,        # Hat 1\n",
    "    85, 26, 139,       # Hair 2\n",
    "    255, 193, 193,     # Arms 3\n",
    "    0, 255, 255,       # Sunglasses 4\n",
    "    178, 34, 34,       # UpperClothes 5\n",
    "    237, 104, 37,      # LowerClothes 6\n",
    "    32, 178, 170,      # Skirt 7\n",
    "    247, 209, 60,      # Face 8\n",
    "    139, 10, 80        # Dhoes 9\n",
    "]\n",
    "\n",
    "if num_classes == 10:\n",
    "    colors = colors_10\n",
    "else:\n",
    "    colors = colors_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_square(img):\n",
    "    s = img.size\n",
    "    w = abs((s[0]-max(s))//2)\n",
    "    h = abs((s[1]-max(s))//2)\n",
    "    padding = (w,h,max(s)-s[0]-w,max(s)-s[1]-h)\n",
    "    return tf.pad(img, padding)\n",
    "\n",
    "\n",
    "def my_transfrom(img, size=448):\n",
    "    img = pad_to_square(img)\n",
    "    img = tf.resize(img,(size,size))\n",
    "    img = tf.to_tensor(img).float()\n",
    "    img = tf.normalize(img,[0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    img = torch.unsqueeze(img,dim=0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(up_conv,self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
    "            nn.Conv2d(ch_in,ch_out,kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, dim = num_classes):\n",
    "        super(UNet,self).__init__()\n",
    "        \n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(ch_in=3,ch_out=64)\n",
    "        self.Conv2 = conv_block(ch_in=64,ch_out=128)\n",
    "        self.Conv3 = conv_block(ch_in=128,ch_out=256)\n",
    "        self.Conv4 = conv_block(ch_in=256,ch_out=512)\n",
    "        self.Conv5 = conv_block(ch_in=512,ch_out=1024)\n",
    "\n",
    "        self.Up5 = up_conv(ch_in=1024,ch_out=512)\n",
    "        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=512,ch_out=256)\n",
    "        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\n",
    "        \n",
    "        self.Up3 = up_conv(ch_in=256,ch_out=128)\n",
    "        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\n",
    "        \n",
    "        self.Up2 = up_conv(ch_in=128,ch_out=64)\n",
    "        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64,dim,kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n",
    "                m.weight.data.normal_(0, sqrt(2. / n))\n",
    "                nn.init.constant_(m.bias,0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "        \n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        d5 = self.Up5(x5)\n",
    "        d5 = torch.cat((x4,d5),dim=1)\n",
    "        \n",
    "        d5 = self.Up_conv5(d5)\n",
    "        \n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_classes == 10:\n",
    "    unet = UNet(dim=10).cuda()\n",
    "    unet.load_state_dict(torch.load('UNm_10000III_20e_FewerClasses.pth'))\n",
    "else:\n",
    "    unet = UNet(dim=20).cuda()\n",
    "    unet.load_state_dict(torch.load('UNm_10000_30e_MoreAug.pth'))\n",
    "unet.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture('exercise.mp4')\n",
    "frames = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    rgb_frame = frame[:, :, ::-1]\n",
    "    rgb_frame = my_transfrom(Image.fromarray(rgb_frame))   \n",
    "    output = unet(rgb_frame.cuda())\n",
    "    output = torch.max(output,1)[1]\n",
    "    output = output.cpu().clone()\n",
    "    output = torch.as_tensor(output,dtype = torch.uint8)\n",
    "    output = ToPILImage()(output)\n",
    "    output.putpalette(colors)\n",
    "    output = output.convert('RGB')\n",
    "    frames.append(output)\n",
    "\n",
    "imageio.mimsave('exercise-'+str(num_classes)+'cl.gif', frames, 'GIF', duration = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
