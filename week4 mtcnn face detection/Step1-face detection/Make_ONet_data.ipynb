{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate O-Net train&eval data list\n",
    "The purpose of this notebook is to get a txt file in the format of<br><br>\n",
    "\n",
    "`image path`||`label`+`x1` `y1` `x2` `y2`+`x1^` `y1^` `x2^` `y2^`|`label`+`x1` `y1` `x2` `y2`+`x1^` `y1^` `x2^` `y2^`......\\n<br>\n",
    "`image path`||`label`+`x1` `y1` `x2` `y2`+`x1^` `y1^` `x2^` `y2^`|`label`+`x1` `y1` `x2` `y2`+`x1^` `y1^` `x2^` `y2^`......\\n<br>\n",
    "......<br>\n",
    "\n",
    "Here, `x1^` `y1^` `x2^` `y2^` is the coordinate of a valid predicted box generated my P-Net and R-Net and `x1` `y1` `x2` `y2` is the coordinate of a \"true box\" defined to be a ground truth box that is closest to the predicted box. With these information written in a txt file, when O-Net is loading data, the offset can be calculated and the image can be cropped from the image instantaneously. <br>\n",
    "\n",
    "example:<br>\n",
    "```\n",
    "/kaggle/input/wider-data/WIDER/WIDER_train/0--Parade/0_Parade_marchingband_1_849.jpg||n+449 330 571 479+59 428 1013 1383|n+449 330 571 479+934 714 1018 798|...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "from numpy.random import uniform\n",
    "import PIL\n",
    "from PIL import Image, ImageDraw\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get widerface data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root =  '/kaggle/input/wider-data'\n",
    "setdic = {'train':(12876,'WIDER/WIDER_train'),\n",
    "           'val':(3226,'WIDER/WIDER_val'), \n",
    "           'test': (16097,'WIDER/WIDER_test')}\n",
    "\n",
    "def get_all_data(phase):\n",
    "\n",
    "    def is_valid_image(path):\n",
    "        try:\n",
    "            i = Image.open(path)\n",
    "            valid = True\n",
    "        except:\n",
    "            valid = False\n",
    "        return valid\n",
    "    \n",
    "    file = open('/kaggle/input/-mytxt/'+ phase +'.txt')\n",
    "    result = []\n",
    "    gts = []\n",
    "   \n",
    "    lines = file.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        idx = line.find('|')\n",
    "        name = line[:idx]\n",
    "        faces = line[idx+1:].split(',')\n",
    "        image_full_path = os.path.join(root,setdic[phase][1],name)\n",
    "\n",
    "        if is_valid_image(image_full_path):\n",
    "            boxes2D = []\n",
    "            for b in faces:\n",
    "                b = b.split()[:4]\n",
    "                boxes2D.append([int(i) for i in b])\n",
    "            result.append([image_full_path, boxes2D])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-Net and R-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "        \n",
    "\n",
    "class P_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(P_Net, self).__init__()\n",
    "        self.pre_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 10, kernel_size=3, stride=1),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "            nn.Conv2d(10, 16, kernel_size=3, stride=1),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.conv4_1 = nn.Conv2d(32, 1, kernel_size=1, stride=1)\n",
    "        self.conv4_2 = nn.Conv2d(32, 4, kernel_size=1, stride=1)\n",
    "        self.conv4_3 = nn.Conv2d(32, 10, kernel_size=1, stride=1,)\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layer(x)\n",
    "        det = torch.sigmoid(self.conv4_1(x))\n",
    "        box = self.conv4_2(x)\n",
    "        landmark = self.conv4_3(x)\n",
    "        return det, box, landmark\n",
    "\n",
    "    \n",
    "class R_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(R_Net, self).__init__()\n",
    "        self.pre_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 28, kernel_size=3, stride=1),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(28, 48, kernel_size=3, stride=1),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(48, 64, kernel_size=2, stride=1),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        self.conv4 = nn.Linear(64 * 2 * 2, 128)\n",
    "        self.prelu4 = nn.PReLU()\n",
    "        self.conv5_1 = nn.Linear(128, 1)\n",
    "        self.conv5_2 = nn.Linear(128, 4)\n",
    "        self.conv5_3 = nn.Linear(128, 10)\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layer(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.conv4(x)\n",
    "        x = self.prelu4(x)\n",
    "        det = torch.sigmoid(self.conv5_1(x))\n",
    "        box = self.conv5_2(x)\n",
    "        landmark = self.conv5_3(x)\n",
    "        return det, box, landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R_Net(\n",
       "  (pre_layer): Sequential(\n",
       "    (0): Conv2d(3, 28, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): PReLU(num_parameters=1)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(28, 48, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (4): PReLU(num_parameters=1)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(48, 64, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (7): PReLU(num_parameters=1)\n",
       "  )\n",
       "  (conv4): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (prelu4): PReLU(num_parameters=1)\n",
       "  (conv5_1): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (conv5_2): Linear(in_features=128, out_features=4, bias=True)\n",
       "  (conv5_3): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = get_all_data('train')\n",
    "evalset = get_all_data('val')\n",
    "\n",
    "pnet=P_Net().cuda()\n",
    "pnet.load_state_dict(torch.load('/kaggle/input/trainedmtcnn/PNet3.pth'))\n",
    "pnet.eval()\n",
    "rnet=R_Net().cuda()\n",
    "rnet.load_state_dict(torch.load('/kaggle/input/trainedmtcnn/RNet_sl3.pth'))\n",
    "rnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(box, boxes):\n",
    "\n",
    "    box_area = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)\n",
    "    area = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n",
    "    xx1 = np.maximum(box[0], boxes[:, 0])\n",
    "    yy1 = np.maximum(box[1], boxes[:, 1])\n",
    "    xx2 = np.minimum(box[2], boxes[:, 2])\n",
    "    yy2 = np.minimum(box[3], boxes[:, 3])\n",
    "\n",
    "    w = np.maximum(0, xx2 - xx1 + 1)\n",
    "    h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "    inter = w * h\n",
    "    ovr = np.true_divide(inter,(box_area + area - inter))\n",
    "    return ovr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_first_stage(image, net, scale, threshold):\n",
    "\n",
    "    width, height = image.size\n",
    "    sw, sh = math.ceil(width * scale), math.ceil(height * scale)\n",
    "    img = image.resize((sw, sh), Image.BILINEAR)\n",
    "    img = transforms.ToTensor()(img).unsqueeze(0)\n",
    "    img = img.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "    output = net(img)\n",
    "    probs = output[0].data.cpu().numpy()[0, 0, :, :]\n",
    "    offsets = output[1].data.cpu().numpy()\n",
    "    boxes = _generate_bboxes(probs, offsets, scale, threshold)\n",
    "    if len(boxes) == 0:\n",
    "        return None\n",
    "\n",
    "    keep = nms(boxes[:, 0:5], overlap_threshold=0.5)\n",
    "    return boxes[keep]\n",
    "\n",
    "\n",
    "def _generate_bboxes(probs, offsets, scale, threshold):\n",
    "    stride = 2\n",
    "    cell_size = 12\n",
    "\n",
    "    inds = np.where(probs > threshold)\n",
    "    if inds[0].size == 0:\n",
    "        return np.array([])\n",
    "    tx1, ty1, tx2, ty2 = [offsets[0, i, inds[0], inds[1]] for i in range(4)]\n",
    "\n",
    "    offsets = np.array([tx1, ty1, tx2, ty2])\n",
    "    score = probs[inds[0], inds[1]]\n",
    "\n",
    "    bounding_boxes = np.vstack([\n",
    "        np.round((stride * inds[1] + 1.0) / scale),\n",
    "        np.round((stride * inds[0] + 1.0) / scale),\n",
    "        np.round((stride * inds[1] + 1.0 + cell_size) / scale),\n",
    "        np.round((stride * inds[0] + 1.0 + cell_size) / scale),\n",
    "        score, offsets\n",
    "    ])\n",
    "    return bounding_boxes.T\n",
    "\n",
    "\n",
    "def nms(boxes, overlap_threshold=0.5, mode='union'):\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    scores = boxes[:, 4]\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "\n",
    "        if mode is 'min':\n",
    "            ovr = inter / np.minimum(areas[i], areas[order[1:]])\n",
    "        else:\n",
    "            ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= overlap_threshold)[0]\n",
    "        order = order[inds + 1]\n",
    "    return keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_square(bboxes):\n",
    "    square_bboxes = np.zeros_like(bboxes)\n",
    "    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n",
    "    h = y2 - y1 + 1.0\n",
    "    w = x2 - x1 + 1.0\n",
    "    max_side = np.maximum(h, w)\n",
    "    square_bboxes[:, 0] = x1 + w * 0.5 - max_side * 0.5\n",
    "    square_bboxes[:, 1] = y1 + h * 0.5 - max_side * 0.5\n",
    "    square_bboxes[:, 2] = square_bboxes[:, 0] + max_side - 1.0\n",
    "    square_bboxes[:, 3] = square_bboxes[:, 1] + max_side - 1.0\n",
    "    return square_bboxes\n",
    "\n",
    "\n",
    "def calibrate_box(bboxes, offsets):\n",
    "\n",
    "    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n",
    "    w = x2 - x1 + 1.0\n",
    "    h = y2 - y1 + 1.0\n",
    "    w = np.expand_dims(w, 1)\n",
    "    h = np.expand_dims(h, 1)\n",
    "\n",
    "    translation = np.hstack([w, h, w, h]) * offsets\n",
    "    bboxes[:, 0:4] = bboxes[:, 0:4] + translation\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def get_image_boxes(bounding_boxes, img, size=24):\n",
    "    num_boxes = len(bounding_boxes)\n",
    "    width, height = img.size\n",
    "\n",
    "    [dy, edy, dx, edx, y, ey, x, ex, w, h] = correct_bboxes(bounding_boxes, width, height)\n",
    "    img_boxes = np.zeros((num_boxes, 3, size, size), 'float32')\n",
    "\n",
    "    for i in range(num_boxes):\n",
    "        img_box = np.zeros((h[i], w[i], 3), 'uint8')\n",
    "        img_array = np.asarray(img, 'uint8')\n",
    "        img_box[dy[i]:(edy[i] + 1), dx[i]:(edx[i] + 1), :] = \\\n",
    "            img_array[y[i]:(ey[i] + 1), x[i]:(ex[i] + 1), :]\n",
    "        img_box = Image.fromarray(img_box)\n",
    "        img_box = img_box.resize((size, size), Image.BILINEAR)\n",
    "        img_box = np.asarray(img_box, 'float32')\n",
    "        img_boxes[i, :, :, :] = img_normalization(img_box)\n",
    "\n",
    "    return img_boxes\n",
    "\n",
    "\n",
    "def correct_bboxes(bboxes, width, height):\n",
    "    x1, y1, x2, y2 = [bboxes[:, i] for i in range(4)]\n",
    "    w, h = x2 - x1 + 1.0, y2 - y1 + 1.0\n",
    "    num_boxes = bboxes.shape[0]\n",
    "\n",
    "    x, y, ex, ey = x1, y1, x2, y2\n",
    "    dx, dy = np.zeros((num_boxes,)), np.zeros((num_boxes,))\n",
    "    edx, edy = w.copy() - 1.0, h.copy() - 1.0\n",
    "\n",
    "    ind = np.where(ex > width - 1.0)[0]\n",
    "    edx[ind] = w[ind] + width - 2.0 - ex[ind]\n",
    "    ex[ind] = width - 1.0\n",
    "\n",
    "    ind = np.where(ey > height - 1.0)[0]\n",
    "    edy[ind] = h[ind] + height - 2.0 - ey[ind]\n",
    "    ey[ind] = height - 1.0\n",
    "\n",
    "    ind = np.where(x < 0.0)[0]\n",
    "    dx[ind] = 0.0 - x[ind]\n",
    "    x[ind] = 0.0\n",
    "\n",
    "    ind = np.where(y < 0.0)[0]\n",
    "    dy[ind] = 0.0 - y[ind]\n",
    "    y[ind] = 0.0\n",
    "    return_list = [dy, edy, dx, edx, y, ey, x, ex, w, h]\n",
    "    return_list = [i.astype('int32') for i in return_list]\n",
    "\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def img_normalization(img):\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    img = np.expand_dims(img, 0)\n",
    "    img = (img - 127.5) * 0.0078125\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The threshold for R-Net is set to 0.03 in order to collect more boxes refined by R-Net in the O-Net dataset.<br>\n",
    "Since the thresholds for P-Net have been set to 0.85 and 0.5 when P-Net generates data for R-Net, they should be kept consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLDS = [0.85,0.03]\n",
    "NMS_THRESHOLDS = [0.5,0.9]\n",
    "MIN_FACE_SIZE = 15.0\n",
    "\n",
    "\n",
    "def pnet_boxes(img, pnet, min_face_size=MIN_FACE_SIZE, thresholds=THRESHOLDS, nms_thresholds=NMS_THRESHOLDS):\n",
    "    pnet.eval()\n",
    "    width, height = img.size\n",
    "    min_length = min(height, width)\n",
    "    min_detection_size = 12\n",
    "    factor = 0.707\n",
    "    scales = []\n",
    "    m = min_detection_size / min_face_size\n",
    "    min_length *= m\n",
    "    factor_count = 0\n",
    "    while min_length > min_detection_size:\n",
    "        scales.append(m * factor ** factor_count)\n",
    "        min_length *= factor\n",
    "        factor_count += 1\n",
    "\n",
    "    bounding_boxes = []\n",
    "    for s in scales:\n",
    "        boxes = run_first_stage(img, pnet, scale=s, threshold=thresholds[0])\n",
    "        bounding_boxes.append(boxes)\n",
    "\n",
    "    bounding_boxes = [i for i in bounding_boxes if i is not None]\n",
    "\n",
    "    try:\n",
    "        _ = bounding_boxes[0]\n",
    "    except Exception:\n",
    "        img.show()\n",
    "    if len(bounding_boxes) == 0:\n",
    "        return None\n",
    "    bounding_boxes = np.vstack(bounding_boxes)\n",
    "\n",
    "    keep = nms(bounding_boxes[:, 0:5], nms_thresholds[0])\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes = calibrate_box(bounding_boxes[:, 0:5], bounding_boxes[:, 5:])\n",
    "    bounding_boxes = convert_to_square(bounding_boxes)\n",
    "    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnet_boxes(img, rnet, bounding_boxes, thresholds=THRESHOLDS, nms_thresholds=NMS_THRESHOLDS):\n",
    "    rnet.eval()\n",
    "    img_boxes = get_image_boxes(bounding_boxes, img, size=24)\n",
    "    img_boxes = torch.FloatTensor(img_boxes)\n",
    "    img_boxes=img_boxes.cuda()\n",
    "    output = rnet(img_boxes)\n",
    "    probs = output[0].data.cpu().numpy()\n",
    "    offsets = output[1].data.cpu().numpy()\n",
    "\n",
    "    keep = np.where(probs[:, 0] > thresholds[1])[0]\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes[:, 4] = probs[keep, 0].reshape((-1,))\n",
    "    offsets = offsets[keep]\n",
    "\n",
    "    keep = nms(bounding_boxes, nms_thresholds[1])\n",
    "    bounding_boxes = bounding_boxes[keep]\n",
    "    bounding_boxes = calibrate_box(bounding_boxes, offsets[keep])\n",
    "    bounding_boxes = convert_to_square(bounding_boxes)\n",
    "    bounding_boxes[:, 0:4] = np.round(bounding_boxes[:, 0:4])\n",
    "    return bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "global DEVICE\n",
    "DEVICE = torch.device('cuda:0')\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "else:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## start running\n",
    "### generate training data for O-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 0\n",
      "working on 100\n",
      "working on 200\n",
      "working on 300\n",
      "working on 400\n",
      "working on 500\n",
      "working on 600\n",
      "working on 700\n",
      "working on 800\n",
      "working on 900\n",
      "working on 1000\n",
      "working on 1100\n",
      "working on 1200\n",
      "working on 1300\n",
      "working on 1400\n",
      "working on 1500\n",
      "working on 1600\n",
      "working on 1700\n",
      "working on 1800\n",
      "working on 1900\n",
      "working on 2000\n",
      "working on 2100\n",
      "working on 2200\n",
      "working on 2300\n",
      "working on 2400\n",
      "working on 2500\n",
      "working on 2600\n",
      "working on 2700\n",
      "working on 2800\n",
      "working on 2900\n",
      "working on 3000\n",
      "working on 3100\n",
      "working on 3200\n",
      "working on 3300\n",
      "working on 3400\n",
      "working on 3500\n",
      "working on 3600\n",
      "working on 3700\n",
      "working on 3800\n",
      "working on 3900\n",
      "working on 4000\n",
      "working on 4100\n",
      "working on 4200\n",
      "working on 4300\n",
      "working on 4400\n",
      "working on 4500\n",
      "working on 4600\n",
      "working on 4700\n",
      "working on 4800\n",
      "working on 4900\n",
      "working on 5000\n",
      "working on 5100\n",
      "working on 5200\n",
      "working on 5300\n",
      "working on 5400\n",
      "working on 5500\n",
      "working on 5600\n",
      "working on 5700\n",
      "working on 5800\n",
      "working on 5900\n",
      "working on 6000\n",
      "working on 6100\n",
      "working on 6200\n",
      "working on 6300\n",
      "working on 6400\n",
      "working on 6500\n",
      "working on 6600\n",
      "working on 6700\n",
      "working on 6800\n",
      "working on 6900\n",
      "working on 7000\n",
      "working on 7100\n",
      "working on 7200\n",
      "working on 7300\n",
      "working on 7400\n",
      "working on 7500\n",
      "working on 7600\n",
      "working on 7700\n",
      "working on 7800\n",
      "working on 7900\n",
      "working on 8000\n",
      "working on 8100\n",
      "working on 8200\n",
      "working on 8300\n",
      "working on 8400\n",
      "working on 8500\n",
      "working on 8600\n",
      "working on 8700\n",
      "working on 8800\n",
      "working on 8900\n",
      "working on 9000\n",
      "working on 9100\n",
      "working on 9200\n",
      "working on 9300\n",
      "working on 9400\n",
      "working on 9500\n",
      "working on 9600\n",
      "working on 9700\n",
      "working on 9800\n",
      "working on 9900\n",
      "working on 10000\n",
      "working on 10100\n",
      "working on 10200\n",
      "working on 10300\n",
      "working on 10400\n",
      "working on 10500\n",
      "working on 10600\n",
      "working on 10700\n",
      "working on 10800\n",
      "working on 10900\n",
      "working on 11000\n",
      "working on 11100\n",
      "working on 11200\n",
      "working on 11300\n",
      "working on 11400\n",
      "working on 11500\n",
      "working on 11600\n",
      "working on 11700\n",
      "working on 11800\n",
      "working on 11900\n",
      "working on 12000\n",
      "working on 12100\n",
      "working on 12200\n",
      "working on 12300\n",
      "working on 12400\n",
      "working on 12500\n",
      "working on 12600\n",
      "working on 12700\n",
      "working on 12800\n",
      "Complete in 77m 12s\n"
     ]
    }
   ],
   "source": [
    "onet_train_data = open('/kaggle/working/onet_train.txt','w')\n",
    "\n",
    "\n",
    "def is_valid_image(path):\n",
    "    try:\n",
    "        i = Image.open(path)\n",
    "        valid = True\n",
    "    except:\n",
    "        print('image open failure +1')\n",
    "        valid = False\n",
    "    return valid\n",
    "\n",
    "iou_th = {'n': (0, 0.3), 'pf': (0.4, 0.65), 'p': (0.65, 1.0)}\n",
    "since = time.time()\n",
    "\n",
    "for num,(img_pth, faces) in enumerate(trainset):    \n",
    "    \n",
    "    if num%100==0:\n",
    "        print('working on '+str(num))\n",
    "    \n",
    "    \n",
    "    if not is_valid_image(img_pth):\n",
    "        continue\n",
    "    image = Image.open(img_pth)\n",
    "    bounding_boxes = pnet_boxes(image, pnet)\n",
    "    if bounding_boxes is None:\n",
    "        continue\n",
    "    bounding_boxes_rnet = rnet_boxes(image, rnet, bounding_boxes)\n",
    "    if len(bounding_boxes_rnet) != 0:\n",
    "        bounding_boxes = np.vstack((bounding_boxes, bounding_boxes_rnet))\n",
    "    \n",
    "        \n",
    "    faces=np.array(faces)\n",
    "    faces[:, 2] += faces[:, 0]\n",
    "    faces[:, 3] += faces[:, 1]\n",
    "    \n",
    "    width, height = image.size\n",
    "    total = len(bounding_boxes)\n",
    "    \n",
    "    onet_train_data.write(img_pth+'|')\n",
    "    for id, box in enumerate(bounding_boxes, start=1):\n",
    "        box = [min(max(0, int(box[i])), width if i % 2 == 0 else height) for i in range(4)]\n",
    "        if box[2] - box[0] < 48: continue\n",
    "        iou = IoU(box, faces)\n",
    "        iou_max = iou.max()\n",
    "        iou_index = iou.argmax()\n",
    "        closet_face = faces[iou_index]\n",
    "        \n",
    "        for temp_label in iou_th:\n",
    "            if iou_max < iou_th[temp_label][0] or iou_max > iou_th[temp_label][1]:\n",
    "                continue\n",
    "            else:\n",
    "                label = temp_label\n",
    "                closet_box = ' '.join([str(i) for i in closet_face]) \n",
    "                crop_box = ' '.join([str(i) for i in box]) \n",
    "                \n",
    "                onet_train_data.write('|'+label+'+'+closet_box+'+'+crop_box)\n",
    "    onet_train_data.write('\\n')\n",
    "onet_train_data.close()\n",
    "\n",
    "spent = time.time() - since\n",
    "print('Complete in {:.0f}m {:.0f}s'.format(spent // 60, spent % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate evaluation data for O-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 0\n",
      "working on 100\n",
      "working on 200\n",
      "working on 300\n",
      "working on 400\n",
      "working on 500\n",
      "working on 600\n",
      "working on 700\n",
      "working on 800\n",
      "working on 900\n",
      "working on 1000\n",
      "working on 1100\n",
      "working on 1200\n",
      "working on 1300\n",
      "working on 1400\n",
      "working on 1500\n",
      "working on 1600\n",
      "working on 1700\n",
      "working on 1800\n",
      "working on 1900\n",
      "working on 2000\n",
      "working on 2100\n",
      "working on 2200\n",
      "working on 2300\n",
      "working on 2400\n",
      "working on 2500\n",
      "working on 2600\n",
      "working on 2700\n",
      "working on 2800\n",
      "working on 2900\n",
      "working on 3000\n",
      "working on 3100\n",
      "working on 3200\n",
      "Complete in 18m 20s\n"
     ]
    }
   ],
   "source": [
    "onet_eval_data = open('/kaggle/working/onet_eval.txt','w')\n",
    "\n",
    "iou_th = {'n': (0, 0.3), 'pf': (0.4, 0.65), 'p': (0.65, 1.0)}\n",
    "since = time.time()\n",
    "\n",
    "for num,(img_pth, faces) in enumerate(evalset):  \n",
    "    \n",
    "    if num%100==0:\n",
    "        print('working on '+str(num))\n",
    "    \n",
    "    if not is_valid_image(img_pth):\n",
    "        continue\n",
    "    image = Image.open(img_pth)\n",
    "    bounding_boxes = pnet_boxes(image, pnet)\n",
    "    if bounding_boxes is None:\n",
    "        continue\n",
    "    bounding_boxes_rnet = rnet_boxes(image, rnet, bounding_boxes)\n",
    "    if len(bounding_boxes_rnet) != 0:\n",
    "        bounding_boxes = np.vstack((bounding_boxes, bounding_boxes_rnet))\n",
    "    \n",
    "    faces=np.array(faces)\n",
    "    faces[:, 2] += faces[:, 0]\n",
    "    faces[:, 3] += faces[:, 1]\n",
    "    \n",
    "    width, height = image.size\n",
    "    total = len(bounding_boxes)\n",
    "    \n",
    "    onet_eval_data.write(img_pth+'|')\n",
    "    for id, box in enumerate(bounding_boxes, start=1):\n",
    "        box = [min(max(0, int(box[i])), width if i % 2 == 0 else height) for i in range(4)]\n",
    "        if box[2] - box[0] < 48: continue\n",
    "        iou = IoU(box, faces)\n",
    "        iou_max = iou.max()\n",
    "        iou_index = iou.argmax()\n",
    "        closet_face = faces[iou_index]\n",
    "        \n",
    "        for temp_label in iou_th:\n",
    "            if iou_max < iou_th[temp_label][0] or iou_max > iou_th[temp_label][1]:\n",
    "                continue\n",
    "            else:\n",
    "                label = temp_label\n",
    "                closet_box = ' '.join([str(i) for i in closet_face]) \n",
    "                crop_box = ' '.join([str(i) for i in box]) \n",
    "                \n",
    "                onet_eval_data.write('|'+label+'+'+closet_box+'+'+crop_box)\n",
    "    onet_eval_data.write('\\n')\n",
    "onet_eval_data.close()\n",
    "\n",
    "spent = time.time() - since\n",
    "print('Complete in {:.0f}m {:.0f}s'.format(spent // 60, spent % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
